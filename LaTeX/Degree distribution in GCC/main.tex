\documentclass[a4paper]{article}

\input{used_packages.tex}
\input{custom_commands.tex}

\title{Degree distribution in GCC}
\author{BenoÃ®t Richard \and Guiyuan Shi}

\addbibresource{biblio.bib}

\begin{document}

\listoftodos

\maketitle

\abstract{\todo[inline]{Write abstract}}

\section{Introduction}

Studying the fundamental properties of networks require to be able to abstract from the particular examples found in nature. This is usually done \missingref by using a random model for the network generation and averaging the properties of interest over the set of possible networks. One common model is the configuration model \missingref that allows to uniformly sample the space of all networks with a given degree distribution \missingref. However, many real examples of networks are connected, as for example the World Wide Web or railroad networks\todo{Find other examples}, but no model known to us allows to uniformly sample the space of all connected networks of a given degree distribution.

A way to still study connected networks is to only consider the Giant Connected Components (GCC) of networks generated using the configuration model \missingref. We study here how this method implies bias on the degree distribution of the GCC and propose an algorithm based on this knowledge to generate connected networks of arbitrary degree distribution.

\section{Degree distribution in GCC}

Per Bayes theorem we have for two random events $A$ and $B$
\begin{align}
	P(A | B) = \frac{P(A \cap B)}{ P(B)} = P(B | A) \frac{P(A)}{P(B)}. \label{Bayes theorem}
\end{align}
We can apply it to compute the probability $r_k$ that a vertex in the GCC has degree $k$
\begin{align}
	r_k &= P\left(deg(v) = k | v \in GCC\right)\\
	&= P(v \in GCC | deg(v) = k) \frac{P(deg(v) = k)}{P(v \in GCC)} \\
	&= (1 - P(v \notin GCC|deg(v) = k)) \frac{p_k}{S} \\
	&= \frac{p_k}{S} (1 - u^k). \label{Degree distribution in GCC},
\end{align}
where $S$ is the probability that a random node is part of the GCC, $p_k$ is the probability that a node has degree $k$ and $u$ is the probability that a node reached by following an edge of the network is not part of the GCC. Justification of the equality $P(v \notin GCC|deg(v) = k) = u^k$ can be found inf Ref. \citep{newman2010networks}.

In the context of the configuration model we choose the probabilities $p_k$ which determine $u$ and $S$ through the following equations \cite{newman2010networks}
\begin{align}
	u &= \frac{\sum_{k=1}^\infty k p_k u^{k-1}}{\sum_{k=1}^\infty k p_k} \\ \label{Expression for u}
	S &= 1 - \sum_{k=1}^\infty p_k u^{k},
\end{align}
thus eliminating all unknown in eq. \eqref{Degree distribution in GCC}.

As seen in eq. \eqref{Degree distribution in GCC}, considering a vertex in the GCC biases the probability that it has degree $k$ by a factor $(1 - u^k)/S$ as compared to choosing a vertex uniformly in the network. The bias factor is depicted in fig. \ref{Figure: low degree saturation}. Since both $u$ and $S$ are smaller than $1$, the net effect is to lower the proportion of low degree vertices in the GCC and thus to increase the proportion of high degree vertices.

\begin{figure}
	\includegraphics[width=0.8\textwidth]{drift_term.pdf}
	\caption{Bias factor $(1 - u^k)/S$ for Poisson random graph of various mean degree $c$. The closer to the critical point at $c = 1$, the larger the effect.}
	\label{Figure: low degree saturation}
\end{figure}

\section{Generating connected networks}

The knowledge of the degree distribution in the GCC can be used generate a connected component of a given degree distribution $r_k$ as follow: we first determine a degree distribution $p_k$ fulfilling eq. \eqref{Degree distribution in GCC} for some target degree distribution $r_k$. Then we generate a network with degree distribution $p_k$ using the configuration model. Finally we take its GCC as our connected network. By construction the vertices in the GCC will have degree distribution $r_k$. Determining the factors $p_k$ is not immediate however since $u$ is an unknown which is itself a function of $p_k$. We propose an algorithm to determine it numerically.

First we isolate $p_k$ from eq. \eqref{Degree distribution in GCC} to get
\begin{align}
	p_k &= S \pi_k(u), \quad \text{with} \quad \pi_k(z) = \frac{r_k}{1 - z^k}
\end{align}
Inserting this in the expression\eqref{Expression for u} for $u$, we get
\begin{align}
	u &= \frac{\sum_{k=1}^\infty k \pi_k(u) u^{k-1}}{\sum_{k=1}^\infty k \pi_k(u)}.
\end{align}
Therefore $u$ is a fixpoint of the function
\begin{align}
	\mu(z) = \frac{\sum_{k=1}^\infty k \pi_k(z) z^{k-1}}{\sum_{k=1}^\infty k \pi_k(z)}, \label{Defition of mu}
\end{align}
which is fully determined by the GCC degree distribution $r_k$. Note that for $r_1 = 0$, we have the fixpoint $u = 0$ and $p_k = r_k$ for all $k$. This is consistent with the fact that small component of a network produced with the configuration model have a probability $0$ to have loop \cite{newman2010networks}. Indeed if $p_1 = 0$ all components must have loops, therefore the probability to have small components is $0$ as well.


On the other hand $r_1 > 0$ implies $u > 0$. To approximate its value we define the sequence $u_{j+1} = \mu(u_j)$, with $u_0 = r_1$. This sequence will converge toward $u$ for large $j$ \todo{Still no proof of that}. Since we can not deal numerically with infinite sums, we need to choose a cutoff index $K$ such that
\begin{align}
	\sum_{k=K+1}^\infty k \pi_k(u) \ll 1.
\end{align}

For scale-free network with exponent between ?? and ??\todo{Find correct exponent}, this sum always diverges and thus this method is not applicable.

Once $u$ is approximated, we can compute the first $K$ probabilities $p_k$, which is sufficient to sample random numbers between $1$ and $K$ with relative probability $p_k$. If $K$ is chosen such that $r_k << 1$ for $k > K$, the degree distribution in the GCC closely approximate the distribution $r_k$.

\begin{figure}
	\missingfigure{Plot showing residuals on the resulting $p_k$ compared to expected one.}
	 \caption{Do the caption as well}
\end{figure}

\todo[inline]{Would be interesting to have a use case where the difference in distribution has a meaningful impact}

\section{Discussion}

\todo[inline]{Write discussion}

\appendix
\section{Analysis of the $\mu(z)$ function}
\label{Appendix: Fixpoint convergence}

To prove that $\lim_{z -> 1} \mu'(z) > 1$ , we compute its derivative with respect to $z$, which yields
\begin{align}
	\mu'(z) &= \left[\sum_{k = 1}^{\infty}k \pi_k(z)\right]^{-2} \left(s_1(z) + s_2(z)\right) \\
	s_1(z) &= \sum_{j, k}k j \pi_k'(z) \pi_j(z) \left( z^{k-1} -  z^{j-1}\right) \\
	s_2(z) &= \sum_{j, k} k (k - 1) j \pi_k(z) \pi_j(z) z^{k-2}.
\end{align}
The sum $s_1(z)$ can be rewritten as
\begin{align}
	s_1(z) &= \sum_{j > k} k j \left(\pi_k'(z) \pi_j(z) - \pi_j'(z) \pi_k(z)\right) \left(z^{k-1} -  z^{j-1}\right) \\
		&=\sum_{j > k} \frac{k r_k}{1 - z^k} \frac{j r_j}{1 - z^j} \frac{z^k - z^j}{z^2} \left(\frac{k}{z^{-k} - 1} - \frac{j}{z^{-j} - 1}\right)\\
		&=\sum_{j > k} k j \pi_k(z) \pi_j(z) \frac{z^k - z^j}{z^2} \left(\frac{k}{z^{-k} - 1} - \frac{j}{z^{-j} - 1}\right).
\end{align}
Using the fact that the function
\begin{align}
	f_z(\lambda) = \frac{\lambda}{z^{-\lambda} - 1}
\end{align}
is a decreasing function of $\lambda$ we can see that for $z \in [0, 1)$ and $j > k$ we have
\begin{align}
	z^k - z^j &\geq 0 \\
	\frac{k}{z^{-k} - 1} - \frac{j}{z^{-j} - 1} &\geq 0,
\end{align}
and thus $s_1(z) \geq 0$. Moreover each terms in we have $s_2(z) > 0$, so we can conclude that $\mu'(z) > 0$ and thus that $\mu(z)$ is a strictly increasing function of $z$.

In order to prove that $\mu(z) > z$, we first rewrite the function $\mu(z)$ as
\begin{align}
	\mu(z) = \frac{1}{z} \left[1 - \frac{\sum_{k=1}^\infty k r_k}{\sum_{k=1}^\infty \frac{k r_k}{1 - z^k}}\right].
\end{align}
Noting that since $0 \leq z < 1$ we have $1 - z^2 \leq 1 - z^k$ for $k \geq 2$, we find
\todo[inline]{Verify the following}
\begin{align}
	\mu(z) &\leq \frac{1}{z} \left[1 - (1 - z^2)\frac{\sum_{k=1}^\infty k r_k}{(1 + z)r_1 + \sum_{k=2}^\infty k r_k}\right].
\end{align}

\printbibliography{}

\end{document}